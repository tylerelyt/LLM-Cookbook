# -*- coding: utf-8 -*-
"""
Context Engineering Prompt Builder - åŸºäºä¸Šä¸‹æ–‡å·¥ç¨‹çš„Promptæ„å»ºå™¨

æœ¬æ¨¡å—å®ç°äº†ä¸€ä¸ªå…ˆè¿›çš„Promptæ„å»ºç³»ç»Ÿï¼Œå°†Promptåˆ’åˆ†ä¸ºå››ä¸ªæ ¸å¿ƒåˆ†åŒºï¼š
1. äººè®¾è®°å¿† (Persona Memory) - AIçš„èº«ä»½å’Œè¡Œä¸ºç‰¹å¾
2. å·¥å…·åˆ†åŒº (Tools Partition) - å¯ç”¨å·¥å…·çš„å®šä¹‰å’Œä½¿ç”¨è¯´æ˜
3. æƒ…èŠ‚è®°å¿† (Episodic Memory) - ä»MemGPTè·å–çš„å¯¹è¯ä¸Šä¸‹æ–‡
4. è¯­ä¹‰è®°å¿† (Semantic Memory) - ä»RAGç³»ç»Ÿæ£€ç´¢çš„çŸ¥è¯†å†…å®¹
"""

import json
import logging
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Any, Optional, Union
from datetime import datetime
from enum import Enum


# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class MemoryType(Enum):
    """è®°å¿†ç±»å‹æšä¸¾"""
    PERSONA = "persona"
    TOOLS = "tools" 
    EPISODIC = "episodic"
    SEMANTIC = "semantic"


@dataclass
class ConversationTurn:
    """å¯¹è¯è½®æ¬¡æ•°æ®ç»“æ„"""
    role: str
    content: str
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class MemoryPartition:
    """è®°å¿†åˆ†åŒºæ•°æ®ç»“æ„"""
    type: MemoryType
    content: str
    priority: int = 1
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)


class MemoryProvider(ABC):
    """è®°å¿†æä¾›è€…æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def retrieve_memory(self, query: str, context: Dict[str, Any]) -> str:
        """æ£€ç´¢è®°å¿†å†…å®¹"""
        pass
    
    @abstractmethod
    def update_memory(self, content: str, metadata: Dict[str, Any]) -> bool:
        """æ›´æ–°è®°å¿†å†…å®¹"""
        pass


class MemGPTProvider(MemoryProvider):
    """MemGPTæƒ…èŠ‚è®°å¿†æä¾›è€…"""
    
    def __init__(self, api_endpoint: Optional[str] = None, max_context_length: int = 4000):
        self.api_endpoint = api_endpoint or "http://localhost:8283"
        self.max_context_length = max_context_length
        self.memory_cache = {}
    
    def retrieve_memory(self, query: str, context: Dict[str, Any]) -> str:
        """ä»MemGPTè·å–æƒ…èŠ‚è®°å¿†"""
        try:
            logger.info(f"[MemGPT] æ­£åœ¨æ£€ç´¢æƒ…èŠ‚è®°å¿†ï¼ŒæŸ¥è¯¢: {query[:50]}...")
            
            # è·å–å¯¹è¯å†å²
            conversation_history = context.get('conversation_history', [])
            
            if not conversation_history:
                return "è¿™æ˜¯å¯¹è¯çš„å¼€å§‹ï¼Œæš‚æ— å†å²è®°å¿†ã€‚"
            
            # æ™ºèƒ½æ‘˜è¦ç”Ÿæˆ
            summary = self._generate_intelligent_summary(conversation_history, query)
            
            # æå–å…³é”®ä¸Šä¸‹æ–‡
            key_contexts = self._extract_key_contexts(conversation_history, query)
            
            # æ„å»ºæƒ…èŠ‚è®°å¿†
            episodic_memory = f"""
## å¯¹è¯æ‘˜è¦
{summary}

## å…³é”®ä¸Šä¸‹æ–‡
{key_contexts}

## æƒ…æ„ŸçŠ¶æ€
{self._analyze_conversation_mood(conversation_history)}
"""
            
            logger.info("[MemGPT] æƒ…èŠ‚è®°å¿†æ£€ç´¢å®Œæˆ")
            return episodic_memory.strip()
            
        except Exception as e:
            logger.error(f"MemGPTè®°å¿†æ£€ç´¢å¤±è´¥: {e}")
            return "æƒ…èŠ‚è®°å¿†æš‚æ—¶ä¸å¯ç”¨ï¼Œå°†åŸºäºå½“å‰å¯¹è¯ç»§ç»­ã€‚"
    
    def _generate_intelligent_summary(self, conversation_history: List[ConversationTurn], current_query: str) -> str:
        """ç”Ÿæˆæ™ºèƒ½å¯¹è¯æ‘˜è¦"""
        if len(conversation_history) <= 3:
            return "å¯¹è¯åˆšå¼€å§‹ï¼Œç”¨æˆ·æ­£åœ¨æ¢ç´¢å’Œäº†è§£ç³»ç»Ÿèƒ½åŠ›ã€‚"
        
        # åˆ†æå¯¹è¯ä¸»é¢˜
        topics = self._extract_topics(conversation_history)
        user_intent = self._analyze_user_intent(conversation_history, current_query)
        
        return f"ç”¨æˆ·ä¸»è¦å…³æ³¨: {', '.join(topics)}ã€‚å½“å‰æ„å›¾: {user_intent}"
    
    def _extract_key_contexts(self, conversation_history: List[ConversationTurn], query: str) -> str:
        """æå–å…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        key_points = []
        
        # è·å–æœ€è¿‘çš„é‡è¦å¯¹è¯
        recent_turns = conversation_history[-5:] if len(conversation_history) > 5 else conversation_history
        
        for turn in recent_turns:
            if len(turn.content) > 20:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„å›å¤
                role_name = "ç”¨æˆ·" if turn.role == "user" else "åŠ©æ‰‹"
                key_points.append(f"- {role_name}: {turn.content[:100]}...")
        
        return "\n".join(key_points) if key_points else "æš‚æ— å…³é”®ä¸Šä¸‹æ–‡ã€‚"
    
    def _extract_topics(self, conversation_history: List[ConversationTurn]) -> List[str]:
        """æå–å¯¹è¯ä¸»é¢˜"""
        # ç®€åŒ–çš„ä¸»é¢˜æå–ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä½¿ç”¨NLPæŠ€æœ¯ï¼‰
        topics = set()
        keywords = ["RAG", "MemGPT", "AI", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†", "æç¤ºå·¥ç¨‹"]
        
        for turn in conversation_history:
            for keyword in keywords:
                if keyword.lower() in turn.content.lower():
                    topics.add(keyword)
        
        return list(topics) if topics else ["ä¸€èˆ¬å¯¹è¯"]
    
    def _analyze_user_intent(self, conversation_history: List[ConversationTurn], current_query: str) -> str:
        """åˆ†æç”¨æˆ·æ„å›¾"""
        if "ä»€ä¹ˆæ˜¯" in current_query or "è§£é‡Š" in current_query:
            return "å¯»æ±‚çŸ¥è¯†è§£é‡Š"
        elif "å¦‚ä½•" in current_query or "æ€ä¹ˆ" in current_query:
            return "å¯»æ±‚æ“ä½œæŒ‡å¯¼"
        elif "æ¯”è¾ƒ" in current_query or "åŒºåˆ«" in current_query:
            return "å¯»æ±‚æ¯”è¾ƒåˆ†æ"
        else:
            return "ä¸€èˆ¬è¯¢é—®"
    
    def _analyze_conversation_mood(self, conversation_history: List[ConversationTurn]) -> str:
        """åˆ†æå¯¹è¯æƒ…æ„ŸçŠ¶æ€"""
        if not conversation_history:
            return "ä¸­æ€§"
        
        # ç®€åŒ–çš„æƒ…æ„Ÿåˆ†æ
        last_user_turn = None
        for turn in reversed(conversation_history):
            if turn.role == "user":
                last_user_turn = turn
                break
        
        if last_user_turn:
            content = last_user_turn.content.lower()
            if any(word in content for word in ["è°¢è°¢", "å¾ˆå¥½", "æ£’", "å–œæ¬¢"]):
                return "ç§¯æ"
            elif any(word in content for word in ["ä¸", "é”™", "é—®é¢˜", "å›°æƒ‘"]):
                return "å›°æƒ‘"
        
        return "ä¸­æ€§"
    
    def update_memory(self, content: str, metadata: Dict[str, Any]) -> bool:
        """æ›´æ–°æƒ…èŠ‚è®°å¿†"""
        try:
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨MemGPT API
            logger.info("[MemGPT] æƒ…èŠ‚è®°å¿†å·²æ›´æ–°")
            return True
        except Exception as e:
            logger.error(f"MemGPTè®°å¿†æ›´æ–°å¤±è´¥: {e}")
            return False


class RAGProvider(MemoryProvider):
    """RAGè¯­ä¹‰è®°å¿†æä¾›è€…"""
    
    def __init__(self, knowledge_base_path: Optional[str] = None, embedding_model: str = "default"):
        self.knowledge_base_path = knowledge_base_path
        self.embedding_model = embedding_model
        self.knowledge_cache = {}
        self._init_knowledge_base()
    
    def _init_knowledge_base(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“"""
        self.knowledge_base = {
            "RAG": {
                "definition": "æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯ä¸€ç§AIæŠ€æœ¯ï¼Œå®ƒå°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ä¸å¤–éƒ¨çŸ¥è¯†åº“çš„æ£€ç´¢èƒ½åŠ›ç›¸ç»“åˆã€‚",
                "components": ["æ£€ç´¢å™¨", "ç”Ÿæˆå™¨", "çŸ¥è¯†åº“"],
                "advantages": ["æé«˜å‡†ç¡®æ€§", "å‡å°‘å¹»è§‰", "æ”¯æŒå®æ—¶ä¿¡æ¯"],
                "use_cases": ["é—®ç­”ç³»ç»Ÿ", "å†…å®¹ç”Ÿæˆ", "çŸ¥è¯†ç®¡ç†"]
            },
            "MemGPT": {
                "definition": "MemGPTæ˜¯ä¸€ç§ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›é•¿æœŸè®°å¿†èƒ½åŠ›çš„ç³»ç»Ÿï¼Œé€šè¿‡åˆ†å±‚è®°å¿†æ¶æ„å®ç°æŒä¹…åŒ–å¯¹è¯ä¸Šä¸‹æ–‡ã€‚",
                "features": ["é•¿æœŸè®°å¿†", "ä¸Šä¸‹æ–‡ç®¡ç†", "è‡ªä¸»è®°å¿†æ“ä½œ"],
                "architecture": ["æ ¸å¿ƒè®°å¿†", "å½’æ¡£è®°å¿†", "é€’å½’æ‘˜è¦"],
                "benefits": ["å¯¹è¯è¿è´¯æ€§", "ä¸ªæ€§åŒ–ä½“éªŒ", "çŸ¥è¯†ç§¯ç´¯"]
            },
            "Context Engineering": {
                "definition": "ä¸Šä¸‹æ–‡å·¥ç¨‹æ˜¯è®¾è®¡å’Œä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹è¾“å…¥æç¤ºçš„æŠ€æœ¯å’Œæ–¹æ³•è®ºã€‚",
                "principles": ["æ˜ç¡®æ€§", "ç»“æ„æ€§", "ç›¸å…³æ€§", "ç®€æ´æ€§"],
                "techniques": ["åˆ†åŒºè®¾è®¡", "ç¤ºä¾‹å­¦ä¹ ", "è§’è‰²è®¾å®š", "ä»»åŠ¡åˆ†è§£"],
                "best_practices": ["ä½¿ç”¨æ¸…æ™°åˆ†éš”ç¬¦", "æä¾›å…·ä½“ç¤ºä¾‹", "æ˜ç¡®æœŸæœ›è¾“å‡ºæ ¼å¼"]
            },
            "Prompt Engineering": {
                "definition": "æç¤ºå·¥ç¨‹æ˜¯è®¾è®¡æœ‰æ•ˆæç¤ºæ¥å¼•å¯¼AIæ¨¡å‹äº§ç”ŸæœŸæœ›è¾“å‡ºçš„è‰ºæœ¯å’Œç§‘å­¦ã€‚",
                "strategies": ["é›¶æ ·æœ¬å­¦ä¹ ", "å°‘æ ·æœ¬å­¦ä¹ ", "æ€ç»´é“¾", "è§’è‰²æ‰®æ¼”"],
                "optimization": ["è¿­ä»£æ”¹è¿›", "A/Bæµ‹è¯•", "æ•ˆæœè¯„ä¼°", "æ¨¡æ¿åŒ–"],
                "challenges": ["æ¨¡å‹å·®å¼‚", "ä»»åŠ¡å¤æ‚åº¦", "è¾“å‡ºä¸€è‡´æ€§", "æˆæœ¬æ§åˆ¶"]
            }
        }
    
    def retrieve_memory(self, query: str, context: Dict[str, Any]) -> str:
        """ä»RAGç³»ç»Ÿæ£€ç´¢è¯­ä¹‰è®°å¿†"""
        try:
            logger.info(f"[RAG] æ­£åœ¨æ£€ç´¢è¯­ä¹‰è®°å¿†ï¼ŒæŸ¥è¯¢: {query[:50]}...")
            
            # è®¡ç®—æŸ¥è¯¢ç›¸å…³æ€§
            relevant_docs = self._semantic_search(query)
            
            # æ„å»ºè¯­ä¹‰è®°å¿†
            if relevant_docs:
                semantic_memory = self._format_retrieved_knowledge(relevant_docs, query)
            else:
                semantic_memory = self._generate_fallback_response(query)
            
            logger.info("[RAG] è¯­ä¹‰è®°å¿†æ£€ç´¢å®Œæˆ")
            return semantic_memory
            
        except Exception as e:
            logger.error(f"RAGè®°å¿†æ£€ç´¢å¤±è´¥: {e}")
            return "è¯­ä¹‰è®°å¿†æš‚æ—¶ä¸å¯ç”¨ï¼Œå°†åŸºäºå·²æœ‰çŸ¥è¯†å›ç­”ã€‚"
    
    def _semantic_search(self, query: str) -> List[Dict[str, Any]]:
        """æ‰§è¡Œè¯­ä¹‰æœç´¢"""
        query_lower = query.lower()
        relevant_docs = []
        
        # å…³é”®è¯åŒ¹é…å’Œç›¸å…³æ€§è¯„åˆ†
        for topic, content in self.knowledge_base.items():
            relevance_score = self._calculate_relevance(query_lower, topic, content)
            if relevance_score > 0.3:  # ç›¸å…³æ€§é˜ˆå€¼
                relevant_docs.append({
                    "topic": topic,
                    "content": content,
                    "relevance": relevance_score
                })
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        relevant_docs.sort(key=lambda x: x["relevance"], reverse=True)
        return relevant_docs[:3]  # è¿”å›æœ€ç›¸å…³çš„3ä¸ªæ–‡æ¡£
    
    def _calculate_relevance(self, query: str, topic: str, content: Dict) -> float:
        """è®¡ç®—æŸ¥è¯¢ä¸æ–‡æ¡£çš„ç›¸å…³æ€§"""
        score = 0.0
        
        # ä¸»é¢˜åŒ¹é…
        if topic.lower() in query:
            score += 1.0
        
        # å†…å®¹åŒ¹é…
        all_text = " ".join([str(v) for v in content.values()]).lower()
        query_words = query.split()
        
        for word in query_words:
            if word in all_text:
                score += 0.2
        
        return min(score, 1.0)  # é™åˆ¶æœ€å¤§åˆ†æ•°ä¸º1.0
    
    def _format_retrieved_knowledge(self, docs: List[Dict], query: str) -> str:
        """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„çŸ¥è¯†"""
        formatted_content = []
        
        for doc in docs:
            topic = doc["topic"]
            content = doc["content"]
            
            formatted_content.append(f"""
### {topic}
**å®šä¹‰**: {content.get('definition', 'N/A')}
**å…³é”®ç‰¹å¾**: {', '.join(content.get('features', content.get('components', content.get('principles', []))))}
**ç›¸å…³æ€§**: {doc['relevance']:.2f}
""")
        
        return "\n".join(formatted_content).strip()
    
    def _generate_fallback_response(self, query: str) -> str:
        """ç”Ÿæˆåå¤‡å“åº”"""
        return f"""
æš‚æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ä¸"{query}"ç›´æ¥ç›¸å…³çš„ä¿¡æ¯ã€‚
å»ºè®®ï¼š
1. å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯
2. æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®
3. è€ƒè™‘ä½¿ç”¨åŒä¹‰è¯è¿›è¡Œæœç´¢
"""
    
    def update_memory(self, content: str, metadata: Dict[str, Any]) -> bool:
        """æ›´æ–°è¯­ä¹‰çŸ¥è¯†åº“"""
        try:
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šæ›´æ–°å‘é‡æ•°æ®åº“
            logger.info("[RAG] çŸ¥è¯†åº“å·²æ›´æ–°")
            return True
        except Exception as e:
            logger.error(f"RAGçŸ¥è¯†åº“æ›´æ–°å¤±è´¥: {e}")
            return False


class AdvancedContextPromptBuilder:
    """é«˜çº§ä¸Šä¸‹æ–‡å·¥ç¨‹Promptæ„å»ºå™¨"""
    
    def __init__(self, 
                 persona_profile: str,
                 tools_definition: str,
                 memgpt_provider: Optional[MemGPTProvider] = None,
                 rag_provider: Optional[RAGProvider] = None):
        """
        åˆå§‹åŒ–é«˜çº§Promptæ„å»ºå™¨
        
        Args:
            persona_profile: AIäººè®¾æè¿°
            tools_definition: å·¥å…·å®šä¹‰
            memgpt_provider: MemGPTè®°å¿†æä¾›è€…
            rag_provider: RAGè®°å¿†æä¾›è€…
        """
        self.persona = persona_profile
        self.tools = tools_definition
        self.memgpt_provider = memgpt_provider or MemGPTProvider()
        self.rag_provider = rag_provider or RAGProvider()
        
        # æ€§èƒ½ç›‘æ§
        self.performance_metrics = {
            "total_prompts": 0,
            "avg_build_time": 0,
            "memory_retrieval_times": []
        }
        
        logger.info("é«˜çº§ä¸Šä¸‹æ–‡å·¥ç¨‹Promptæ„å»ºå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def construct_optimized_prompt(self, 
                                 user_query: str, 
                                 conversation_history: List[Dict[str, str]],
                                 priority_weights: Optional[Dict[str, float]] = None) -> str:
        """
        æ„å»ºä¼˜åŒ–çš„ç»“æ„åŒ–Prompt
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²
            priority_weights: å„åˆ†åŒºä¼˜å…ˆçº§æƒé‡
            
        Returns:
            å®Œæ•´çš„ç»“æ„åŒ–Prompt
        """
        start_time = time.time()
        
        try:
            # è½¬æ¢å¯¹è¯å†å²æ ¼å¼
            converted_history = self._convert_conversation_history(conversation_history)
            
            # è®¾ç½®é»˜è®¤æƒé‡
            weights = priority_weights or {
                "persona": 1.0,
                "tools": 0.8,
                "episodic": 0.9,
                "semantic": 1.0
            }
            
            # å¹¶è¡Œè·å–è®°å¿†å†…å®¹
            memory_partitions = self._retrieve_all_memories(user_query, converted_history, weights)
            
            # æ„å»ºåˆ†åŒºåŒ–Prompt
            structured_prompt = self._build_structured_prompt(user_query, memory_partitions)
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self._update_performance_metrics(time.time() - start_time)
            
            logger.info("ä¼˜åŒ–Promptæ„å»ºå®Œæˆ")
            return structured_prompt
            
        except Exception as e:
            logger.error(f"Promptæ„å»ºå¤±è´¥: {e}")
            return self._build_fallback_prompt(user_query)
    
    def _convert_conversation_history(self, history: List[Dict[str, str]]) -> List[ConversationTurn]:
        """è½¬æ¢å¯¹è¯å†å²æ ¼å¼"""
        converted = []
        for turn in history:
            converted.append(ConversationTurn(
                role=turn.get('role', 'user'),
                content=turn.get('content', ''),
                metadata=turn.get('metadata', {})
            ))
        return converted
    
    def _retrieve_all_memories(self, 
                             query: str, 
                             history: List[ConversationTurn],
                             weights: Dict[str, float]) -> List[MemoryPartition]:
        """å¹¶è¡Œæ£€ç´¢æ‰€æœ‰è®°å¿†åˆ†åŒº"""
        partitions = []
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = {
            'conversation_history': history,
            'query': query,
            'timestamp': datetime.now()
        }
        
        # è·å–æƒ…èŠ‚è®°å¿†
        if weights.get("episodic", 0) > 0:
            episodic_content = self.memgpt_provider.retrieve_memory(query, context)
            partitions.append(MemoryPartition(
                type=MemoryType.EPISODIC,
                content=episodic_content,
                priority=int(weights.get("episodic", 1) * 10)
            ))
        
        # è·å–è¯­ä¹‰è®°å¿†
        if weights.get("semantic", 0) > 0:
            semantic_content = self.rag_provider.retrieve_memory(query, context)
            partitions.append(MemoryPartition(
                type=MemoryType.SEMANTIC,
                content=semantic_content,
                priority=int(weights.get("semantic", 1) * 10)
            ))
        
        # æ·»åŠ å›ºå®šåˆ†åŒº
        partitions.extend([
            MemoryPartition(
                type=MemoryType.PERSONA,
                content=self.persona,
                priority=int(weights.get("persona", 1) * 10)
            ),
            MemoryPartition(
                type=MemoryType.TOOLS,
                content=self.tools,
                priority=int(weights.get("tools", 1) * 10)
            )
        ])
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        partitions.sort(key=lambda x: x.priority, reverse=True)
        return partitions
    
    def _build_structured_prompt(self, query: str, partitions: List[MemoryPartition]) -> str:
        """æ„å»ºç»“æ„åŒ–Prompt"""
        
        prompt_sections = []
        
        # æ·»åŠ Promptå¤´éƒ¨
        prompt_sections.append("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                          CONTEXT-ENGINEERED PROMPT SYSTEM                     â•‘
â•‘                             åŸºäºä¸Šä¸‹æ–‡å·¥ç¨‹çš„æ™ºèƒ½Promptç³»ç»Ÿ                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
        
        # æŒ‰ç±»å‹ç»„ç»‡åˆ†åŒº
        for partition in partitions:
            section_title = self._get_section_title(partition.type)
            section_content = self._format_section_content(partition)
            
            prompt_sections.append(f"""
â”Œâ”€ {section_title} â”€â”
{section_content}
â””â”€ END {partition.type.value.upper()} â”€â”˜
""")
        
        # æ·»åŠ ç”¨æˆ·æŸ¥è¯¢éƒ¨åˆ†
        prompt_sections.append(f"""
â”Œâ”€ ç”¨æˆ·æŸ¥è¯¢ (USER QUERY) â”€â”
{query}
â””â”€ END USER QUERY â”€â”˜

è¯·åŸºäºä»¥ä¸Šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚
""")
        
        return "\n".join(prompt_sections)
    
    def _get_section_title(self, memory_type: MemoryType) -> str:
        """è·å–åˆ†åŒºæ ‡é¢˜"""
        titles = {
            MemoryType.PERSONA: "äººè®¾è®°å¿† (PERSONA MEMORY)",
            MemoryType.TOOLS: "å¯ç”¨å·¥å…· (AVAILABLE TOOLS)", 
            MemoryType.EPISODIC: "æƒ…èŠ‚è®°å¿† (EPISODIC MEMORY)",
            MemoryType.SEMANTIC: "è¯­ä¹‰è®°å¿† (SEMANTIC MEMORY)"
        }
        return titles.get(memory_type, "æœªçŸ¥åˆ†åŒº")
    
    def _format_section_content(self, partition: MemoryPartition) -> str:
        """æ ¼å¼åŒ–åˆ†åŒºå†…å®¹"""
        content = partition.content.strip()
        
        # æ·»åŠ æ—¶é—´æˆ³å’Œä¼˜å…ˆçº§ä¿¡æ¯
        metadata_info = f"[ä¼˜å…ˆçº§: {partition.priority}/10] [æ—¶é—´: {partition.timestamp.strftime('%Y-%m-%d %H:%M:%S')}]"
        
        return f"{metadata_info}\n\n{content}"
    
    def _build_fallback_prompt(self, query: str) -> str:
        """æ„å»ºåå¤‡Prompt"""
        return f"""
ç³»ç»Ÿå‡ºç°å¼‚å¸¸ï¼Œä½¿ç”¨ç®€åŒ–æ¨¡å¼ï¼š

äººè®¾: {self.persona}

å·¥å…·: {self.tools}

ç”¨æˆ·æŸ¥è¯¢: {query}

è¯·å°½åŠ›å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
"""
    
    def _update_performance_metrics(self, build_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics["total_prompts"] += 1
        self.performance_metrics["memory_retrieval_times"].append(build_time)
        
        # è®¡ç®—å¹³å‡æ„å»ºæ—¶é—´
        total_time = sum(self.performance_metrics["memory_retrieval_times"])
        self.performance_metrics["avg_build_time"] = total_time / len(self.performance_metrics["memory_retrieval_times"])
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            "total_prompts_built": self.performance_metrics["total_prompts"],
            "average_build_time_seconds": round(self.performance_metrics["avg_build_time"], 4),
            "last_10_build_times": self.performance_metrics["memory_retrieval_times"][-10:],
            "system_status": "æ­£å¸¸è¿è¡Œ" if self.performance_metrics["avg_build_time"] < 2.0 else "æ€§èƒ½è­¦å‘Š"
        }


# === ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•ä»£ç  ===
def create_demo_scenario():
    """åˆ›å»ºæ¼”ç¤ºåœºæ™¯"""
    
    # 1. å®šä¹‰å¢å¼ºçš„äººè®¾
    enhanced_persona = """
ä½ æ˜¯"æ™ºå¿ƒ"ï¼ˆZhiXinï¼‰ï¼Œä¸€ä¸ªä¸“ä¸šçš„AIæŠ€æœ¯é¡¾é—®å’Œç ”ç©¶åŠ©æ‰‹ã€‚

## æ ¸å¿ƒç‰¹å¾
- æ·±åº¦ä¸“ä¸š: åœ¨AIã€æœºå™¨å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæ‹¥æœ‰æ·±åšçŸ¥è¯†
- å–„äºæ•™å­¦: èƒ½å°†å¤æ‚æ¦‚å¿µç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼è§£é‡Š
- å®ç”¨å¯¼å‘: ä¸ä»…æä¾›ç†è®ºçŸ¥è¯†ï¼Œè¿˜èƒ½ç»™å‡ºå®é™…åº”ç”¨å»ºè®®
- æŒç»­å­¦ä¹ : ä¹äºæ¢ç´¢æ–°æŠ€æœ¯å’Œå‰æ²¿ç ”ç©¶

## äº¤äº’é£æ ¼
- å›ç­”å‡†ç¡®ä¸”æœ‰æ®å¯æŸ¥
- ç»“æ„æ¸…æ™°ï¼Œå±‚æ¬¡åˆ†æ˜
- é€‚æ—¶æä¾›ä»£ç ç¤ºä¾‹å’Œå®è·µå»ºè®®
- é¼“åŠ±ç”¨æˆ·æ·±å…¥æ€è€ƒå’Œæ¢ç´¢

## ä¸“ä¸šé¢†åŸŸ
- å¤§è¯­è¨€æ¨¡å‹æ¶æ„ä¸ä¼˜åŒ–
- æç¤ºå·¥ç¨‹ä¸ä¸Šä¸‹æ–‡è®¾è®¡
- RAGç³»ç»Ÿæ„å»ºä¸ä¼˜åŒ–
- è®°å¿†å¢å¼ºAIç³»ç»Ÿï¼ˆå¦‚MemGPTï¼‰
- å¤šæ¨¡æ€AIåº”ç”¨å¼€å‘
"""
    
    # 2. å®šä¹‰å¢å¼ºçš„å·¥å…·é›†
    enhanced_tools = """
## å¯ç”¨å·¥å…·é›†

### è®¡ç®—å·¥å…·
<tool>
  <name>advanced_calculator</name>
  <description>æ‰§è¡Œå¤æ‚æ•°å­¦è®¡ç®—ï¼Œæ”¯æŒç»Ÿè®¡åˆ†æå’Œæ•°æ®å¤„ç†</description>
  <parameters>
    {
      "expression": "æ•°å­¦è¡¨è¾¾å¼æˆ–ç»Ÿè®¡å‡½æ•°",
      "format": "è¾“å‡ºæ ¼å¼ (decimal/scientific/percentage)"
    }
  </parameters>
  <examples>["calculate_accuracy(tp=85, fp=10, fn=5)", "mean([1,2,3,4,5])"]</examples>
</tool>

### æœç´¢å·¥å…·
<tool>
  <name>web_search</name>
  <description>æœç´¢æœ€æ–°çš„æŠ€æœ¯èµ„è®¯å’Œç ”ç©¶è®ºæ–‡</description>
  <parameters>
    {
      "query": "æœç´¢å…³é”®è¯",
      "source": "æœç´¢æ¥æº (arxiv/github/general)",
      "date_range": "æ—¶é—´èŒƒå›´ (recent/year/all)"
    }
  </parameters>
</tool>

### ä»£ç å·¥å…·
<tool>
  <name>code_analyzer</name>
  <description>åˆ†æå’Œä¼˜åŒ–ä»£ç ï¼Œæä¾›æ”¹è¿›å»ºè®®</description>
  <parameters>
    {
      "code": "è¦åˆ†æçš„ä»£ç ",
      "language": "ç¼–ç¨‹è¯­è¨€",
      "analysis_type": "åˆ†æç±»å‹ (performance/security/style)"
    }
  </parameters>
</tool>

### çŸ¥è¯†å›¾è°±å·¥å…·
<tool>
  <name>knowledge_graph_query</name>
  <description>æŸ¥è¯¢çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“å…³ç³»å’Œæ¦‚å¿µè¿æ¥</description>
  <parameters>
    {
      "entity": "æŸ¥è¯¢å®ä½“",
      "relation_type": "å…³ç³»ç±»å‹",
      "depth": "æŸ¥è¯¢æ·±åº¦"
    }
  </parameters>
</tool>
"""
    
    # 3. åˆå§‹åŒ–æä¾›è€…
    memgpt_provider = MemGPTProvider(api_endpoint="http://localhost:8283")
    rag_provider = RAGProvider()
    
    # 4. åˆ›å»ºé«˜çº§æ„å»ºå™¨
    prompt_builder = AdvancedContextPromptBuilder(
        persona_profile=enhanced_persona,
        tools_definition=enhanced_tools,
        memgpt_provider=memgpt_provider,
        rag_provider=rag_provider
    )
    
    return prompt_builder


def run_comprehensive_demo():
    """è¿è¡Œç»¼åˆæ¼”ç¤º"""
    print("\n" + "="*80)
    print("              é«˜çº§ä¸Šä¸‹æ–‡å·¥ç¨‹Promptæ„å»ºå™¨ - ç»¼åˆæ¼”ç¤º")
    print("="*80)
    
    # åˆ›å»ºæ¼”ç¤ºç¯å¢ƒ
    builder = create_demo_scenario()
    
    # æ¨¡æ‹Ÿå¤æ‚å¯¹è¯å†å²
    complex_conversation = [
        {'role': 'user', 'content': 'ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£ä¸€äº›å…³äºç°ä»£AIæŠ€æœ¯çš„çŸ¥è¯†ã€‚'},
        {'role': 'assistant', 'content': 'ä½ å¥½ï¼æˆ‘æ˜¯æ™ºå¿ƒï¼Œå¾ˆé«˜å…´ä¸ºä½ ä»‹ç»AIæŠ€æœ¯ã€‚ä½ å¯¹å“ªä¸ªæ–¹é¢ç‰¹åˆ«æ„Ÿå…´è¶£ï¼Ÿ'},
        {'role': 'user', 'content': 'æˆ‘å¬è¯´è¿‡RAGæŠ€æœ¯ï¼Œä½†ä¸å¤ªæ˜ç™½å®ƒçš„å·¥ä½œåŸç†ã€‚'},
        {'role': 'assistant', 'content': 'RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€é¡¹é‡è¦æŠ€æœ¯...[è¯¦ç»†è§£é‡Š]'},
        {'role': 'user', 'content': 'é‚£MemGPTå’ŒRAGæœ‰ä»€ä¹ˆå…³ç³»å—ï¼Ÿå®ƒä»¬èƒ½ç»“åˆä½¿ç”¨å—ï¼Ÿ'},
        {'role': 'assistant', 'content': 'MemGPTå’ŒRAGç¡®å®å¯ä»¥å¾ˆå¥½åœ°ç»“åˆ...[è¯¦ç»†è¯´æ˜]'},
    ]
    
    # å½“å‰å¤æ‚æŸ¥è¯¢
    current_query = """
    æˆ‘æƒ³æ·±å…¥äº†è§£å¦‚ä½•åœ¨å®é™…é¡¹ç›®ä¸­å°†MemGPTçš„è®°å¿†ç®¡ç†èƒ½åŠ›ä¸RAGçš„çŸ¥è¯†æ£€ç´¢èƒ½åŠ›ç»“åˆèµ·æ¥ã€‚
    å…·ä½“æ¥è¯´ï¼š
    1. å®ƒä»¬åœ¨æ¶æ„ä¸Šå¦‚ä½•ååŒå·¥ä½œï¼Ÿ
    2. æœ‰å“ªäº›æŠ€æœ¯æŒ‘æˆ˜éœ€è¦è§£å†³ï¼Ÿ
    3. èƒ½å¦æä¾›ä¸€äº›å®ç°å»ºè®®å’Œæœ€ä½³å®è·µï¼Ÿ
    """
    
    # è‡ªå®šä¹‰æƒé‡
    priority_weights = {
        "persona": 0.9,
        "tools": 0.7,
        "episodic": 1.0,  # å¯¹è¯ä¸Šä¸‹æ–‡å¾ˆé‡è¦
        "semantic": 1.0   # æŠ€æœ¯çŸ¥è¯†å¾ˆé‡è¦
    }
    
    print("\nğŸ“ æ„å»ºä¸­...")
    
    # æ„å»ºé«˜çº§Prompt
    advanced_prompt = builder.construct_optimized_prompt(
        user_query=current_query,
        conversation_history=complex_conversation,
        priority_weights=priority_weights
    )
    
    print("\nâœ¨ æ„å»ºå®Œæˆï¼")
    print("\n" + "â”€"*80)
    print("ç”Ÿæˆçš„é«˜çº§ç»“æ„åŒ–Prompt:")
    print("â”€"*80)
    print(advanced_prompt)
    
    # æ˜¾ç¤ºæ€§èƒ½æŠ¥å‘Š
    print("\n" + "â”€"*80)
    print("æ€§èƒ½æŠ¥å‘Š:")
    print("â”€"*80)
    performance = builder.get_performance_report()
    for key, value in performance.items():
        print(f"{key}: {value}")


if __name__ == "__main__":
    try:
        run_comprehensive_demo()
    except KeyboardInterrupt:
        print("\n\næ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­ã€‚")
    except Exception as e:
        logger.error(f"æ¼”ç¤ºè¿è¡Œå‡ºé”™: {e}")
        print(f"\næ¼”ç¤ºå‡ºç°é”™è¯¯: {e}")