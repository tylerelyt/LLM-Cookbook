#!/usr/bin/env python3
"""
Lesson 17 Workshopï¼ˆå•æ–‡ä»¶ï¼Œä»…çœŸå® AutoGen ç‰ˆæœ¬ï¼‰

åŠŸèƒ½ï¼šåŸºäº AutoGen çš„â€œé—æ¼ç‚¹å‘ç° + åæ€æ”¹è¿›â€ç³»ç»Ÿï¼Œè¦æ±‚å¯ç”¨ APIã€‚

ç”¨æ³•ï¼š
    export DASHSCOPE_API_KEY='your-api-key-here'
    export LLM_MODEL='qwen-max'   # å¯é€‰ï¼Œé»˜è®¤ qwen-max
    pip install -r requirements.txt
    python workshop.py --task "ä½ çš„ä»»åŠ¡æ–‡æœ¬"

ä¾èµ–ï¼šè§åŒç›®å½• `requirements.txt`
"""

import argparse
import json
import os
import re
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import numpy as np


# =============== è®°å¿†ç³»ç»Ÿï¼šé—æ¼ç‚¹é•¿æœŸè®°å¿†ï¼ˆè¯­ä¹‰æ£€ç´¢ï¼Œæ‰å¹³å­˜å‚¨ï¼‰ ===============
class OmissionMemory:
    """é—æ¼ç‚¹é•¿æœŸè®°å¿†ç³»ç»Ÿï¼ˆæ‰å¹³å­˜å‚¨ï¼‰ã€‚

    - æŒä¹…åŒ–åˆ°è„šæœ¬åŒç›®å½• JSON æ–‡ä»¶ `reflection_memory.json`
    - ä»…è¯­ä¹‰æ£€ç´¢ï¼ˆDashScope å…¼å®¹ OpenAI æ¥å£ï¼‰ã€‚ä¸å¯ç”¨åˆ™ä¸æ£€ç´¢ï¼ˆè¿”å›ç©ºï¼‰
    - å­˜å‚¨ç»“æ„ä¸ºæ‰å¹³åˆ—è¡¨ï¼šæ¯æ¡ä¸ºä¸€ä¸ªåæ€æ¡ç›®ï¼ˆä¸å†æŒ‰é¢†åŸŸåˆ†å±‚ï¼‰ã€‚
    """

    def __init__(self, memory_file: str = None) -> None:
        # å°†æŒä¹…åŒ–æ–‡ä»¶å›ºå®šåœ¨è„šæœ¬ç›®å½•ï¼Œé¿å…å·¥ä½œç›®å½•å˜åŒ–å¯¼è‡´ä¸¢å¤±
        default_path = os.path.join(os.path.dirname(__file__), "reflection_memory.json")
        self.memory_file = memory_file or default_path
        self.entries: List[Dict] = []
        self.embeddings: Dict[str, np.ndarray] = {}
        self._embedding_client = self._init_embedding_client()
        self.load_memory()

    def _init_embedding_client(self):
        try:
            import openai  # type: ignore

            api_key = os.getenv("DASHSCOPE_API_KEY")
            if not api_key:
                return None

            # ä½¿ç”¨ DashScope å…¼å®¹æ¨¡å¼
            client = openai.OpenAI(
                api_key=api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            )
            return client
        except Exception:
            return None

    def _get_embedding(self, text: str) -> Optional[np.ndarray]:
        if not self._embedding_client:
            return None
        try:
            resp = self._embedding_client.embeddings.create(
                model="text-embedding-v1", input=text
            )
            return np.array(resp.data[0].embedding)
        except Exception:
            return None

    def _text_for_embedding(self, item: Dict) -> str:
        parts = [
            item.get("domain", ""),
            item.get("type", ""),
            item.get("description", ""),
            item.get("suggestion", ""),
        ]
        return " ".join([p for p in parts if p])

    def add_omissions(self, domain: str, omissions: List[Dict]) -> None:
        for omission in omissions:
            entry = {
                "id": str(uuid.uuid4()),
                "timestamp": datetime.now().isoformat(),
                "domain": domain,
                "type": omission.get("type", ""),
                "description": omission.get("description", ""),
                "suggestion": omission.get("suggestion", ""),
            }
            self.entries.append(entry)

            emb_text = self._text_for_embedding(entry)
            emb = self._get_embedding(emb_text)
            if emb is not None:
                self.embeddings[entry["id"]] = emb

        self.save_memory()
        print(f"ğŸ’¾ å·²ä¿å­˜ {len(omissions)} ä¸ªé—æ¼ç‚¹åˆ°é¢†åŸŸï¼š{domain}")

    def get_relevant_omissions(self, query: str, limit: int = 3) -> List[Dict]:
        if not self.entries:
            return []

        # ä»…è¯­ä¹‰å¬å›ï¼ˆè‹¥ embeddings ç¼ºå¤±åˆ™æ‡’åŠ è½½è®¡ç®—ï¼‰
        query_emb = self._get_embedding(query)
        if query_emb is None:
            return []

        scored: List[Tuple[float, Dict]] = []
        for item in self.entries:
            emb = self.embeddings.get(item["id"])  # type: ignore[index]
            if emb is None:
                # æ‡’åŠ è½½ä¸ºè¯¥æ¡è®°å¿†ç”ŸæˆåµŒå…¥
                text = self._text_for_embedding(item)
                emb = self._get_embedding(text)
                if emb is None:
                    continue
                self.embeddings[item["id"]] = emb  # type: ignore[index]
            sim = float(np.dot(query_emb, emb) / (np.linalg.norm(query_emb) * np.linalg.norm(emb)))
            scored.append((sim, item))
        scored.sort(key=lambda x: x[0], reverse=True)
        return [item for _, item in scored[:limit]]

    def save_memory(self) -> None:
        try:
            with open(self.memory_file, "w", encoding="utf-8") as f:
                json.dump(self.entries, f, ensure_ascii=False, indent=2)
        except Exception:
            pass

    def load_memory(self) -> None:
        if not os.path.exists(self.memory_file):
            return
        try:
            with open(self.memory_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            # æ–°ç‰ˆä»…æ”¯æŒæ‰å¹³åˆ—è¡¨
            self.entries = data if isinstance(data, list) else []
        except Exception:
            self.entries = []


memory = OmissionMemory()


# ========================= çœŸå®æ¨¡å¼ï¼ˆAutoGen å¤šæ™ºèƒ½ä½“ï¼‰ =========================
def _autogen_available() -> bool:
    try:
        import autogen  # noqa: F401
        return True
    except Exception:
        return False


def _create_llm_config() -> Optional[Dict]:
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        return None

    model = os.getenv("LLM_MODEL", "qwen-max")
    return {
        "config_list": [
            {
                "model": model,
                "api_key": api_key,
                "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            }
        ],
        "temperature": 0.7,
    }


def _reflection_message(recipient, messages, sender, config):
    content = recipient.chat_messages_for_summary(sender)[-1]["content"]

    # åœ¨åæ€å‰æ£€ç´¢ç›¸å…³å†å²åæ€ï¼Œä½œä¸ºä¸Šä¸‹æ–‡æç¤º
    relevant = memory.get_relevant_omissions(content)
    hints = ""
    if relevant:
        lines = [f"â€¢ {it.get('description', '')}ï¼ˆå»ºè®®ï¼š{it.get('suggestion', '')}ï¼‰" for it in relevant]
        hints = "\nå†å²é—æ¼ç‚¹å‚è€ƒ:\n" + "\n".join(lines)

    return f"""è¯·åˆ†æä»¥ä¸‹å†…å®¹çš„é—æ¼ç‚¹ï¼š

{content}
{hints}

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºé—æ¼ç‚¹åˆ†æï¼š
```json
{{
  "domain": "å†…å®¹ä¸»é¢˜é¢†åŸŸï¼ˆè‡ªåŠ¨è¯†åˆ«ï¼‰",
  "omissions": [
    {{
      "type": "é—æ¼ç±»å‹",
      "description": "å…·ä½“é—æ¼æè¿°",
      "suggestion": "æ”¹è¿›å»ºè®®"
    }}
  ]
}}
```"""


def _extract_omissions_from_reflection(reflection_text: str) -> Tuple[str, List[Dict]]:
    try:
        match = re.search(r"```json\s*(\{.*?\})\s*```", reflection_text, re.DOTALL)
        if not match:
            return "é€šç”¨", []
        data = json.loads(match.group(1))
        domain = data.get("domain", "é€šç”¨")
        omissions = data.get("omissions", [])
        if isinstance(omissions, list):
            return domain, omissions
        return domain, []
    except Exception:
        return "é€šç”¨", []


def run_real_autogen_flow(task: str) -> None:
    from autogen import AssistantAgent, UserProxyAgent  # type: ignore

    llm_config = _create_llm_config()
    if not llm_config:
        raise SystemExit("âŒ æœªæ£€æµ‹åˆ° DASHSCOPE_API_KEYï¼Œè¯·å…ˆè®¾ç½®åå†è¿è¡Œã€‚")

    print("ğŸ”§ ä½¿ç”¨çœŸå® AutoGen åæ€æµç¨‹ï¼ˆDashScope å…¼å®¹ï¼‰")

    user_proxy = UserProxyAgent(
        name="user_proxy",
        human_input_mode="NEVER",
        max_consecutive_auto_reply=2,
        code_execution_config={"use_docker": False},
    )

    # åœ¨ç”Ÿæˆå‰æ£€ç´¢ç›¸å…³å†å²åæ€ï¼Œç»™ç”Ÿæˆä»£ç†ä½œä¸ºç³»ç»Ÿæç¤º
    prior_reflections = memory.get_relevant_omissions(task)
    prior_text = ""
    if prior_reflections:
        prior_text = "å†å²é—æ¼ç‚¹å‚è€ƒï¼š\n" + "\n".join(
            [f"- {it.get('description', '')}ï¼ˆå»ºè®®ï¼š{it.get('suggestion', '')}ï¼‰" for it in prior_reflections]
        )

    generator = AssistantAgent(
        name="generator",
        system_message=(
            "ä½ æ˜¯å†…å®¹ç”Ÿæˆä¸“å®¶ã€‚æ ¹æ®è¦æ±‚ç”Ÿæˆå†…å®¹ï¼Œä¸¥æ ¼é¿å…å†å²åæ€ä¸­æŒ‡å‡ºçš„é—æ¼ç‚¹ã€‚\n" + prior_text
        ).strip(),
        llm_config=llm_config,
    )

    reflector = AssistantAgent(
        name="reflector",
        system_message="ä½ æ˜¯é—æ¼ç‚¹å‘ç°ä¸“å®¶ã€‚åˆ†æå†…å®¹é—æ¼ç‚¹å¹¶è¾“å‡ºJSONæ ¼å¼ç»“æœã€‚",
        llm_config=llm_config,
    )

    user_proxy.register_nested_chats(
        [
            {
                "recipient": reflector,
                "message": _reflection_message,
                "max_turns": 1,
            }
        ],
        trigger=generator,
    )

    result = user_proxy.initiate_chat(generator, message=task, max_turns=2)

    # æå–é—æ¼ç‚¹å¹¶å…¥åº“ï¼ˆä¼˜å…ˆä»åæ€ä»£ç†çš„æ¶ˆæ¯ä¸­æŠ½å–ï¼›è‹¥å¤±è´¥åˆ™å…¨å±€æ‰«æä¸€æ¬¡ï¼‰
    saved = False
    if reflector.name in user_proxy.chat_messages:
        msgs = user_proxy.chat_messages[reflector.name]
        if msgs:
            reflection_text = msgs[-1].get("content", "") if isinstance(msgs[-1], dict) else str(msgs[-1])
            domain, omissions = _extract_omissions_from_reflection(reflection_text)
            if omissions:
                memory.add_omissions(domain, omissions)
                print(f"âœ… å·²ä¿å­˜ {len(omissions)} ä¸ªé—æ¼ç‚¹åˆ°é¢†åŸŸï¼š{domain}")
                saved = True

    if not saved:
        # å…¨é‡æ‰«ææ‰€æœ‰å¯¹è¯æ¶ˆæ¯ï¼Œå¯»æ‰¾ç¬¦åˆç»“æ„çš„ JSON ä»£ç å—
        try:
            for agent_name, msgs in user_proxy.chat_messages.items():
                for msg in msgs[::-1]:
                    content = msg.get("content", "") if isinstance(msg, dict) else str(msg)
                    domain, omissions = _extract_omissions_from_reflection(content)
                    if omissions:
                        memory.add_omissions(domain, omissions)
                        print(f"âœ…ï¼ˆå…¨å±€æ‰«æï¼‰å·²ä¿å­˜ {len(omissions)} ä¸ªé—æ¼ç‚¹åˆ°é¢†åŸŸï¼š{domain}")
                        saved = True
                        break
                if saved:
                    break
        except Exception:
            pass

    print(f"ğŸ“Š å½“å‰åæ€æ¡ç›®æ•°ï¼š{len(memory.entries)}")
    # è¿è¡Œç»“æŸæ—¶æ˜ç¡®æç¤ºå­˜å‚¨ä½ç½®ï¼Œæ–¹ä¾¿ç”¨æˆ·åç»­åŠ è½½å¤ç”¨
    print(f"ğŸ—‚ï¸ åæ€è®°å¿†æŒä¹…åŒ–æ–‡ä»¶ï¼š{memory.memory_file}")


# ================================== CLI ==================================
def main() -> None:
    parser = argparse.ArgumentParser(description="Lesson 17 å•æ–‡ä»¶ Workshopï¼ˆçœŸå® AutoGen ç‰ˆï¼‰")
    parser.add_argument(
        "--task",
        type=str,
        default=(
            "å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸåº”ç”¨çš„æ–‡ç« ï¼Œè¦æ±‚ï¼š\n- 500å­—å·¦å³\n- åŒ…å«å…·ä½“åº”ç”¨æ¡ˆä¾‹\n- è®¨è®ºæŒ‘æˆ˜å’Œå‰æ™¯"
        ),
        help="è‡ªå®šä¹‰ä»»åŠ¡æ–‡æœ¬",
    )
    args = parser.parse_args()

    print("ğŸš€ å¯åŠ¨ Lesson 17 Workshopï¼ˆçœŸå® AutoGen ç‰ˆï¼‰")

    if not _autogen_available():
        raise SystemExit(
            "âŒ æœªå®‰è£… autogen-agentchatï¼Œè¯·å…ˆæ‰§è¡Œ: pip install -r requirements.txt"
        )

    if not os.getenv("DASHSCOPE_API_KEY"):
        raise SystemExit(
            "âŒ æœªæ£€æµ‹åˆ° DASHSCOPE_API_KEYï¼Œè¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡åå†è¿è¡Œã€‚"
        )

    run_real_autogen_flow(args.task)


if __name__ == "__main__":
    main()


